{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#To check the encoding of the CSV in order to avoid issue which reading CSV using Pandas.\n",
    "\n",
    "import chardet\n",
    "with open('/kaggle/input/sms-spam-collection-dataset/spam.csv', 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/kaggle/input/sms-spam-collection-dataset/spam.csv',encoding='Windows-1252')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Remove the unwanted columns from dataset and change the label v1 and v2 to label and message respectively\n",
    "data=data[['v1','v2']]\n",
    "data.columns=['label','message']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "wn=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for i in range(0,len(data)):\n",
    "    \n",
    "    review=re.sub('[^a-zA-Z]',' ', data['message'][i])\n",
    "    review.lower()\n",
    "    reviews=review.split(' ')\n",
    "    review=[wn.lemmatize(word) for word in reviews if  not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    \n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#using Bag Of Word\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "X.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.get_dummies(data['label'])\n",
    "y=y.iloc[:,1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train Test Split\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test, y_train , y_test=train_test_split(X,y,test_size=0.3,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection Naive Bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model=MultinomialNB().fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusionM=confusion_matrix(y_test,y_pred)\n",
    "confusionM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=confusionM[0][0]\n",
    "FP=confusionM[0][1]\n",
    "FN=confusionM[1][0]\n",
    "TN=confusionM[1][1]\n",
    "\n",
    "\n",
    "\n",
    "accuracy=((TP + TN)/(TP+FP+FN+TN)) * 100\n",
    "print(accuracy)\n",
    "\n",
    "#Precision tells us how many of the correctly predicted cases actually turned out to be positive.\n",
    "precision=(TP/(TP +FP) ) * 100\n",
    "print(precision)\n",
    "\n",
    "#Recall tells us how many of the actual positive cases we were able to predict correctly with our model.\n",
    "recall= (TP / (TP + FN)) * 100\n",
    "print(recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Or we can use direct library avaiable to get accuracy of the prediction.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracyscore=accuracy_score(y_test,y_pred)\n",
    "accuracyscore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
